[[profiling-self-managed-ops]]
= Operating the backend

[[profiling-self-managed-ops-sizing-guidance]]
== Sizing guidance

The amount of resources needed to ingest and query Universal Profiling data varies with the total number of CPU cores profiled.
The number of cores is the sum of all _virtual_ cores as recorded in `/proc/cpuinfo`, adding up all the machines where the host-agent will be deployed to.

Both ingestion and query resource demand are almost linearly correlated with the amount of data generated by the agents.
Calculating the amount of data generated by the agents is not trivial though.
This is because a portion of the data is almost predicatble (the number of CPU samples collected), but the other portion is not (the number of executables processed and the size of their debug metadata).

The following table provides guidance for the amount of resources needed to ingest and query Universal Profiling data.

|====
| # of CPU cores | Elasticsearch total memory | Elasticsearch total storage (60 days retention) | Profiling Backend | Kibana memory

| 1-100 | 4GB - 8GB | 250GB | 1 Collector 2GB, 1 Symbolizer 2GB | 2GB
| 100-1000 | 8GB - 32GB | 250GB - 2TB | 1 Collector 4GB, 1 Symbolizer 4GB | 2GB
| 1000-10000 | 32GB - 128GB | 2TB - 8TB | 2 Collector 4GB, 1 Symbolizer 8GB | 4GB
| 10000-50000 | 128GB - 512GB | 8TB - 16TB | 3+ Collector 4GB, 1 Symbolizer 8GB | 8GB
|====

If your use case is not present in the table, it is still possible to derive a sizing configuration from the table by comparing the actual number of cores profiled with the number of cores in the table.
When performing such calculations, you should factor in:

. the average load of the machines being profiled
. the rate of change of the executables being profiled (e.g. how often do you deploy new versions of your software)

The average load directly impacts the amount of CPU samples collected, as with a system that is moslty idle, not all CPUs will be scheduling tasks on the sampling intervals.
The rate of change impacts the amount of debug metadata stored in Elasticsearch as a result of symbolization; the more different executables the host-agent collects, the more debug data will be stored in Elasticsearch.
Note that two different builds of the same application still result into two different executables, as the host-agent will treat each ELF file independently.

A note on compute resources: the table above is derived from benchmarks performed on Universal Profiling with ingestion up to 15000 CPU cores.
The profiled machines were having an almost-constant load of 75% CPU utilization.
The deployment used 3 Elasticsearch nodes, with 64 GB memory each, 8 vCPU and 1.5 TB NVMe disk drives.

Storage considerations: the Elasticsearch disks' bandwidth and latency will affect the latency of ingesting and querying the profiling data.
Allocate data to hot nodes for best performance and user experience.
If storage becomes a concern, tune the data retention by customizing the Universal Profiling <<profiling-ilm-custom-policy,"ILM policy">>.

[[profiling-self-managed-ops-configuration]]
== Configuration

Both collector and symbolizer can be configured via YAML file and CLI flags, with the latter taking precedence over the former.
The configuration files are created during the installation process, as seen in the <<profiling-self-managed-running-linux-configfile,"Create configuration files section">>.
The files contain comments that explain the purpose of each configuration option.

Currently, modifying the configuration file requires a restart of the backend binaries.

[[profiling-self-managed-ops-configuration-cli-overrides]]
=== Overriding config values via CLI flags

When building configuration options for each of the backend binaries, you can use CLI flags to override the values in the YAML configuration file.
The overrides **must** contain the full path to the configuration option and must be in a key=value format, e.g. `-E application.field.key=value`, where `application` is the name of the binary.

For example, to enable TLS in the HTTP server of the collector, you can pass the `-E pf-elastic-collector.ssl.enabled=true` flag.
This will override the `ssl.enabled` option found in the YAML configuration file.

[[profiling-self-managed-ops-monioring]]
== Monitoring

Collector and symbolizer should be monitored to ensure they are running and healthy. Without both services running, profiling data will not be ingested and symbolized,
and querying Kibana will result in no data returned.

[[profiling-self-managed-ops-monioring-logs]]
=== Logs

Both applications will always log to standard output.
You can turn on debug logs by setting the `verbose` configuration option to `true` in the YAML configuration file.

Normally, you should not use debug logs in production, as they can be very verbose and impact the performance of the backend.
Enable them only when troubleshooting a failed deployment or when instructed to do so by Support.

The logs are formatted as "key=value" pairs, and can be automatically parsed by Elasticsearch and Kibana into fields.

Logs can be collected by a log collector, such as Filebeat, and sent to Elasticsearch for indexing and analysis.
Depending on the installation method, a filebeat input of type `journald` (for OS packages), `log` (for binaries) or `container` can be used to process the logs.
Refer to the https://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html[filebeat documentation] for more information.

[[profiling-self-managed-ops-monioring-metrics]]
=== Metrics

Both applications can expose metrics in JSON as well as in the Prometheus format.
Metrics are not exposed by default, and you should enable them via the `metrics` section in the YAML configuration files.

The metric in JSON format can be exposed through an HTTP server as well as a Unix Domain Socket.
The Prometheus metrics can only ve exposed on an HTTP server.
For both formats, you can customize where the metrics will be exposed through the `metrics.prometheus_host` and `metrics.expvar_host` configuration options.

You can scrape metrics from any of the two formats via metricbeat, consuming the JSON directly via `http` module, or the Prometheus endpoint with the `prometheus` module.
For both formats, if an HTTP server is used the URI to scrape metrics from will be `/metrics`.

For example, the following configuration for collector will expose metrics in Prometheus format on port 9090 and in JSON format on port 9191,
so you can scrape them connecting to `http://127.0.0.1:9090/metrics` and `http://127.0.0.1:9191/metrics` respectively.

[source,yaml]
----
pf-elastic-collector:
  metrics:
    prometheus_host: ":9090"
    expvar_host: ":9191"
----

Optionally, you can also expose the `expvar` format over a Unix Domain Socket, by setting the `expvar_socket` configuration option to a valid path.
For example, the following configuration for collector will expose metrics in Prometheus format on port 9090,
and in JSON format over a Unix Domain Socket at `/tmp/collector.sock`.

[source,yaml]
----
pf-elastic-collector:
  metrics:
    prometheus_host: ":9090"
    expvar_host: "/tmp/collector.sock"
----


Find below a list of the most relevant metrics exposed by the backend binaries, you should include them in your monitoring dashboards to detect issues with the backend.

==== Common runtime metrics

. `process_cpu_seconds_total`: track the amount of CPU time used by the process.
. `process_resident_memory_bytes`: track the amount of RAM used by the process.
. `go_memstats_heap_sys_bytes`: track the amount of heap memory.
. `go_memstats_stack_sys_bytes`: track the amount of stack memory.
. `go_threads`: number of OS threads created by the runtime.
. `go_goroutines`: number of active goroutines.

==== Collector metrics

. `collection_agent.indexing.bulk_indexer_failure_count`: number of times the bulk indexer failed to ingest data in Elasticsearch.
. `collection_agent.indexing.document_count.*`: counter that represents the number of documents ingested in Elasticsearch for each index; can be used to calculate the rate of ingestion for each index.
. `grpc_server_handling_seconds`: histogram of the time spent by the gRPC server to handle requests.
. `grpc_server_msg_received_total: count of messages received by the gRPC server; can be used to calculate the rate of ingestion for each RPC.
. `grpc_server_handled_total`: count of messages processed by the gRPC server; can be used to calculate the availability of the gRPC server for each RPC.

==== Symbolizer metrics

. `symbols_app.indexing.bulk_indexer_failure_count`: number of times the bulk indexer failed to ingest data in Elasticsearch.
. `symbols_app.indexing.document_count.*`: counter that represents the number of documents ingested in Elasticsearch for each index; can be used to calculate the rate of ingestion for each index.
. `symbols_app.user_client.document_count.update.*`: counter that represents the number of existing documents that were updated in Elasticsearch for each index; when rate increases, it can impact Elasticsearch performance.

=== Health checks

The backend binaries expose two health check endpoint that can be used to monitor the health of the application.
The health checks are `/live` and `/ready`, and they return a 200 OK HTTP status code when the checks are successful.

The health check endpoints are hosted in the same HTTP server accepting the incoming profiling data.
This endpoint is configured through the application's `host` configuration option.

For example, if collector is configured with the default value `host: 0.0.0.0:8260`, you can check the health of the application by running `curl -i localhost:8260/live` and `curl -i localhost:8260/ready`.

== Scaling resources

In the sizing guidance table above there is not a single option that uses more than one replica for the symbolizer.
We don't recommend scaling the number of symbolizer replicas, due to technical limitations of the current implementation.
At the moment, the best option is to scale the symbolizer vertically, by increasing the memory and CPU cores it uses to process data.

For collector, you can increase the number of collector replicas at will, keeping their vertical sizing smaller, if this is more convenient for your deployment use case.
The collector has a linear increase in memory usage and CPU threads with the number of host-agents that it serves.
Keep in mind that since the host-agent/collector communication happens via gRPC, there may be long-lived TCP sessions that are bound to a single collector replica.
When scaling out the number of replicas, depending on the load balancer that you have in place fronting the collector's endpoint, you may want to shut down the older replicas after adding new replicas.
This is to ensure that the load is evenly distributed across all replicas.

== Upgrading

An upgrade to the backend binaries should be applied whenever an upgrade is applied to the rest of the Elastic stack.
While we try to keep backward compatibility between two consecutive minor version, we may introduce changes to the data format that require the applications to be at the same version of Elasticsearch and Kibana.

The upgrade process step vary depending on the installation method used.

=== ECE

When using ECE, the upgrade process is managed by the platform itself.
You don't need to perform any action to upgrade the backend binaries.

=== Kubernetes

Perform an upgrade via Helm using the `helm upgrade` command.
You may re-use existing values or provide the full values YAML file on each upgrade.

=== OS packages

You should upgrade the package version using the OS package manager.
Keep in mind that not all package managers will call into `systemd` to restart the service, so you may need to restart the service manually or via any other automation in place.

=== Binaries

Download the corresponding binary version and replace the existing one, using the command seen in the <<profiling-self-managed-running-linux-binary,"Binary">> section of the setup guide.
Replace the old binary and restart the services.

=== Containers

Pull the new container image and replace the existing one, using the new image.
